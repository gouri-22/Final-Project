# -*- coding: utf-8 -*-
"""chatWithDocuments_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y9hr4Tvl71cx7Ha6U40jHYAsWoe_bR9S
"""

pip install langchain pdfminer.six unstructured chromadb langchain_community langchain_google_genai python-dotenv

!pip install pyngrok

!pip install gradio

!pip install -U langchain-huggingface sentence-transformers

import os
from dotenv import load_dotenv

api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    with open('.env', 'w') as f:
        f.write("GOOGLE_API_KEY = ")
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")

os.environ["GOOGLE_API_KEY"] = api_key

from langchain.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from pdfminer.high_level import extract_text
from langchain.text_splitter import CharacterTextSplitter

def process_pdf(file_path):
    pdf_text = extract_text(file_path)
    text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)
    texts = text_splitter.split_text(pdf_text)
    return texts

def process_text(file_path):
    with open(file_path, 'r') as file:
        text = file.read()
    text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)
    texts = text_splitter.split_text(text)
    return texts

def create_vectorstore(texts):
    embeddings = HuggingFaceEmbeddings()
    vectorstore = Chroma.from_texts(texts, embeddings)
    return vectorstore

from langchain_google_genai import GoogleGenerativeAI
from langchain.chains import RetrievalQA

def create_qa_chain(vectorstore):
    llm = GoogleGenerativeAI(model = "gemini-pro", temperature = 0.7)
    qa_chain = RetrievalQA.from_chain_type(
        llm = llm,
        chain_type = "stuff",
        retriever = vectorstore.as_retriever()
    )
    return qa_chain

def process_file(file_path):
    if file_path.lower().endswith('.pdf'):
        texts = process_pdf(file_path)
    elif file_path.lower().endswith('.txt'):
        texts = process_text(file_path)
    else:
        raise ValueError("Unsupported file type")
    vectorstore = create_vectorstore(texts)
    qa_chain = create_qa_chain(vectorstore)
    return qa_chain

def handle_query(qa_chain, query):
    response = qa_chain.run(query)
    return response

from google.colab import files
import gradio as gr

def gradio_app(file, query):
    qa_chain = process_file(file)
    response = handle_query(qa_chain, query)
    return response

with gr.Blocks() as app:
    gr.HTML("""
    <div style="text-align: center; background-color: #f7f9f6; padding: 20px;">
        <h1>Chat with Documents using Gemini</h1>
    </div>
    """)

    with gr.Row():
        with gr.Column():
            file_input = gr.File(label="Upload a PDF or Text File")
            query_input = gr.Textbox(label="Enter your query")
            submit_button = gr.Button("Submit")
            output_text = gr.Textbox(label="Response", interactive=False)

    submit_button.click(fn=gradio_app, inputs=[file_input, query_input], outputs=output_text)

app.launch(share=True, inbrowser=True, server_name="0.0.0.0", server_port=7860)





