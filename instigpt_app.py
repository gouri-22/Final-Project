# -*- coding: utf-8 -*-
"""instigpt_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STqug0HeNOYmiD5yHfN9Qk2dcfu2bu4Q
"""

!pip install "unstructured[all-docs]" langchain tiktoken chromadb faiss-cpu

!pip install gradio

!pip install langchain-google-genai

!pip install pdfminer.six

!pip install sentence-transformers

!pip install -U langchain-huggingface

!pip install langchain-community

!pip installÂ python-dotenv

from google.colab import files
uploaded_pdf = files.upload()



from unstructured.partition.pdf import partition_pdf
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory

with open('.env', 'w') as f:
    f.write("GOOGLE_API_KEY = ")

import os
from dotenv import load_dotenv
load_dotenv()

api_key = os.getenv("GOOGLE_API_KEY")
os.environ["GOOGLE_API_KEY"] = api_key

#Extracting data from the pdf Instigpt.pdf using the pdfminer library
from pdfminer.high_level import extract_text
pdf_text = extract_text("Instigpt.pdf")

from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_text(pdf_text)

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
embeddings = HuggingFaceEmbeddings()
vectorstore = Chroma.from_texts(texts, embeddings)

from langchain_google_genai import GoogleGenerativeAI
llm = GoogleGenerativeAI(model = "gemini-pro", temperature = 0.7)
memory = ConversationBufferMemory(return_messages=True)

qa_chain = RetrievalQA.from_chain_type(
    llm = llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(),
    memory =  memory,
)

def chatbot(query):
    response = qa_chain.run(query)
    return response

queries = [
    "What is the KReSIT canteen popular for?",
    "Describe the SMP.",
    "How to join a club in the institute?"
]

for query in queries:
    print(f"Query: {query}")
    response = chatbot(query)
    print(f"Response: {response}\n")

import gradio as gr

def gradio_interface(query):
    return chatbot(query)

with gr.Blocks() as app:
    gr.HTML("<h1 style='text-align: center; color: darkgreen;'>INSTIGPT</h1>")

    with gr.Row():
        with gr.Column():
            input_text = gr.Textbox(label="Ask a Question", placeholder="Type your question here...")
            output_text = gr.Textbox(label="Response", interactive=False)

            def process_input(text):
                return chatbot(text)

            input_text.submit(process_input, inputs=input_text, outputs=output_text)

    gr.HTML("""
    <style>
        .gradio-container {
            background-color: #e0f2f1;
            color: #004d40;
        }
    </style>
    """)

app.launch()